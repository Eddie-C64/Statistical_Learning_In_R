scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +
scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +
geom_point(color = I('blue'), alpha = 1/4)
ggplot(data = diamonds,aes( x = carat, y = price)) +
scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +
scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +
geom_point(color = I('blue'), alpha = 1/4) +
stat_smooth(method = 'lm', formula = price ~ log(carat), se = F)
```
ggplot(data = diamonds,aes( x = carat, y = price)) +
scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +
scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +
geom_point(color = I('blue'), alpha = 1/4) +
stat_smooth(method = 'lm', formula = diamonds$price ~ log(diamonds$carat), se = F)
library(GGally)
library(ggplot2)
library(scales)
library(memisc)
library(lattice)
library(MASS)
library(car)
library(reshape)
library(plyr)
data(diamonds)
ggplot(data = diamonds,aes( x = carat, y = price)) +
scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +
scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +
geom_point(color = I('blue'), alpha = 1/4) +
stat_smooth(method = 'lm', formula = diamonds$price ~ log(diamonds$carat), se = F)
install.packages("car")
install.packages("scale")
install.packages("scales")
install.packages("scales")
install.packages("scales")
install.packages("scales")
install.packages("scales")
install.packages("memisc")
install.packages("MASS")
install.packages("reshape")
install.packages("plyr")
install.packages("plyr")
library(GGally)
library(ggplot2)
library(scales)
library(memisc)
library(lattice)
library(MASS)
library(car)
library(reshape)
library(plyr)
data(diamonds)
ggplot(data = diamonds,aes( x = carat, y = price)) +
scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99))) +
scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99))) +
geom_point(color = I('blue'), alpha = 1/4) +
stat_smooth(method = 'lm', formula = diamonds$price ~ log(diamonds$carat), se = F)
library(GGally)
library(ggplot2)
library(scales)
library(memisc)
library(lattice)
library(MASS)
library(car)
library(reshape)
library(plyr)
library(gridExtra)
plot1 <- qplot(data = diamonds, x = carat, y = price) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = log(carat), y = price) +
ggtitle('Price (log10)')
grid.arrange()
install.packages("gridExtra")
library(gridExtra)
plot1 <- qplot(data = diamonds, x = carat, y = price) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = log(carat), y = price) +
ggtitle('Price (log10)')
grid.arrange()
library(gridExtra)
plot1 <- qplot(data = diamonds, x = price) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = price) +
ggtitle('Price (log10)')
+ scale_x_log10()
grid.arrange()
install.packages(grid)
install.packages("grid")
install.packages("grid")
library(gridExtra)
library(grid)
plot1 <- qplot(data = diamonds, x = price) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = price) +
ggtitle('Price (log10)')
+ scale_x_log10()
grid.arrange(plot1, plot2, ncol = 2)
plot1 <- qplot(data = diamonds, x = price, binwidth = 100, fill = I('#099DD9')) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = price, binwidth = 0.01, fill = I('#F79420')) +
ggtitle('Price (log10)')
+ scale_x_log10()
grid.arrange(plot1, plot2, ncol = 2)
library(gridExtra)
library(grid)
plot1 <- qplot(data = diamonds, x = price, binwidth = 100, fill = I('#099DD9')) +
ggtitle('Price')
plot2 <- qplot(data = diamonds, x = price, binwidth = 0.01, fill = I('#F79420')) +
ggtitle('Price (log10)') +
scale_x_log10()
grid.arrange(plot1, plot2, ncol = 2)
qplot(data = diamonds, x = price, y = price) +
scale_y_continuous(trans = log10_trans()) +
ggtitle('Price Log(10) by Carat')
plot(x, col=(3-y))
set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1, ] + 1
plot(x, col=(3-y))
install.packages('e1071')
plot(x, col=(3-y))
dat=data.frame(x=x,y=as.factor(y))
library(e1071)
svmfit=svm(y~., data = dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit, dat)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
summary(svmfit)
svmfit=svm(y~., data = dat, kernel='linear', cost=0.1, scale=FALSE)
plot(svmfit, dat)
svmfit$index
set.seed(1)
tune.out = tune(svnm, y~., data=dat, kernel="linear", ranges = list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
set.seed(1)
tune.out = tune(svm, y~., data=dat, kernel="linear", ranges = list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
bestmod = tune.out$best.model
summary(bestmod)
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(-1,1), 20, rep=TRUE)
xtest[y==1,]=xtest[ytest==1, ] + 1
testdat = data.frame(x=xtest, y=as.factor(ytest))
ypred = predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
bestmod = tune.out$best.model
summary(bestmod)
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1, ] + 1
testdat = data.frame(x=xtest, y=as.factor(ytest))
ypred = predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
View(dat)
View(dat)
install.packages('MASS')
install.packages('ISLR')
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
names(lm.fit)
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
cat('\014')
attach(Boston)
lm.fit = lm(medv~lstat, data=Boston)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit, data.frame(lstat(5, 10, 15)), interval = "confidence")
coef(lm.fit)
confint(lm.fit)
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval = "confidence")
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval = "prediction")
plot(lstat, medv)
abline(lm.fit)
abline(lm.fit, lwd=3)
abline(lm.fit, lwd=3, col="red")
plot(lstat, medv)
abline(lm.fit) # Plot with Regression Line
abline(lm.fit, lwd=3) ## Make line thicker
abline(lm.fit, lwd=3, col="red") ## Make the line thicker and red!
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(lstat, medv, pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
library(MASS)
library(ISLR)
fix(Boston)
lm.fit ~ lm(medc~lstat+age, data=Boston)
summary(lm.fit)
lm.fit ~ lm(medc~lstat+age, data=Boston)
summary(lm.fit)
lm.fit ~ lm(medc~lstat+age, data=Boston)
summary(lm.fit)
lm.fit ~ lm(medv~lstat+age, data=Boston)
summary(lm.fit)
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
vif(lm.fit)
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
library(cars)
vif(lm.fit)
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
library(car)
vif(lm.fit)
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
library(car)
vif(lm.fit)
lm.fit.1 = lm(medv~.-age, data = Boston)
summary(lm.fit.1)
lm.non.linear = lm(medv~lstat+I(lstat^2), data = Boston)
summary(lm.non.linear)
lm.fit = lm(medv~lstat)
anova(lm.fit, lm.non.linear)
lm.fit = lm(medv~lstat, data = Boston)
anova(lm.fit, lm.non.linear)
lm.fit = lm(medv~lstat, data = Boston)
anova(lm.fit, lm.non.linear)
par(mfrow-c(2,2))
plot(lm.non.linear)
anova(lm.fit, lm.non.linear)
par(mfrow=c(2,2))
plot(lm.non.linear)
lm.fit = lm(medv~lstat, data = Boston)
anova(lm.fit, lm.non.linear)
par(mfrow=c(2,2))
plot(lm.non.linear)
lm.fit.3 = lm(mdev~poly(lstat,5))
summary(lm.fit.3)
lm.fit.3 = lm(medv~poly(lstat,5))
summary(lm.fit.3)
lm.fit.3 = lm(medv~poly(lstat,5), data = Boston)
summary(lm.fit.3)
fix(Carseats)
names(Carseats)
fix(Carseats)
names(Carseats)
lm.fit = lm(Sales~.+Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)
LoadLibraries = function(){
library(ISLR)
+ library(MASS)
+ print("The libraries have been loaded.")
}
LoadLibraries()
print("The libraries have been loaded.")
library(Cars)
library(car)
attach(cars)
lm.fit = lm(mpg~horsepower, data = cars)
set.seed(2000)
x=rnorm(100)
y=2*x+rnorm(100)
lm.fit = lm(y~x+0)
summary(lm.fit)
('/014')
lm.fit.no.intercept = lm(y~x+0)
summary(lm.fit.no.intercept)
lm.fit.with.intercept = lm(y~x)
summary(lm.fit.with.intercept)
('/014')
lm.fit.no.intercept = lm(y~x+0)
summary(lm.fit.no.intercept)
lm.fit.with.intercept = lm(y~x)
summary(lm.fit.with.intercept)
#comparision
anova(lm.fit.no.intercept, lm.fit.with.intercept)
anova(lm.fit.no.intercept, lm.fit.with.intercept)
t.test(y~x)
setwd("/Users/lalo/CodingComplication/DataScienceProjects/Statistical_Learning_In_R/Decision_Trees")
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+57)/200
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(94+60)/200
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+62)/200
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
set.seed(2)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
# Fitting Regression Trees
set.seed(3)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
set.seed(2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
set.seed(2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
set.seed(3)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
setwd("/Users/lalo/CodingComplication/DataScienceProjects/Statistical_Learning_In_R/Decision_Trees")
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(MASS)
library(tree)
library(ISLR)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=250)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=2500)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
